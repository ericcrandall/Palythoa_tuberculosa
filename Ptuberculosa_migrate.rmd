---
title: "Gene Flow Models for *Palythoa tuberculosa*"
output: 
  html_document:
    toc: true
    toc_float: true 
  html_notebook:
    toc: true
    toc_float: true
bibliography: "./Ptuberculosa.bib"
---

# Setup

```{r setup, warning = F, message = F}
library(tidyverse)
library(ape)
library(phangorn)
library(pegas)
library(strataG)
library(knitr)
library(emo)

# Function library
harvest.model.likelihoods <- function(workingDir = workingDir,
                                      outfileName = "outfile.txt",
                                      multilocus = T){
    # this function harvests model marginal likelihoods for models calculated by
    # the program migrate-n (Beerli & Felsenstein 2001).
    # It takes as input a directory full of directories, 
    # each of which contains output from a migrate model, and is named
    # after that model. 
  
    #initialize a data frame to take the values
    modelMarglikes <- data.frame(model=character(),
                             thermodynamic=numeric(),
                             bezier.corrected=numeric(), 
                             harmonic=numeric()) 
    # loop through directories in the working directory, each of which is name
    # after a different model
  for(i in list.dirs(workingDir, full.names = F)[-1]){ #i<-"stepping.stone"
      modelDir<-file.path(workingDir,i)
      print(modelDir)
    #scan in the outfile, separating at each newline
      outfile<-scan(file=file.path(modelDir,outfileName),what="character",sep="\n") 
    #find the line with the likelihoods on it and split on runs of spaces
      marglikeline <- strsplit(grep("  All          ",outfile,value=T),
                               "\\s+", perl = T)[[1]][3:5]
    #  if(length(marglikeline)==0){next}
      marglikes <- c(i,marglikeline)
     
      modelMarglikes <- rbind(modelMarglikes,marglikes, deparse.level = 2)
  }
  names(modelMarglikes) <- c("model","thermodynamic","bezier.corrected","harmonic")
  return(modelMarglikes)
}

bfcalcs<-function(df,ml="bezier.corrected"){
  # This calculates log bayes factors on data frames output by
  # harvest.model.likelihoods(), following Johnson and Omland (2004)
  # You may choose the likelihood flavor with
  # ml = "bezier.corrected", "thermodynamic" or "harmonic"
  df$thermodynamic <- as.numeric(df$thermodynamic)
  df$bezier.corrected <- as.numeric(df$bezier.corrected)
  df$harmonic <- as.numeric(df$harmonic)
    mlcol <- df[,ml] 
	bmvalue <- mlcol[which.max(mlcol)]
	lbf <- 2*(mlcol-bmvalue)
	choice <- rank(-mlcol)
	modelprob <- exp(lbf/2)/sum(exp(lbf/2))
	dfall <- cbind(df,lbf,choice,modelprob)
	return(dfall)
}	


```

# Introduction

Rob Toonen asked me to take a look at this dataset sequenced and analyzed by 'Ale'alani Dudoit. 

From Rob:  

> It is a zoanthid - Palythoa tuberculosa - that is most common in anthropogenically disturbed habitats.  It seems like it could be a cool story, and she has plenty of SNPs or contigs to do whatever analysis you think would be most interesting to work up.

'Ale'a:

> Mahalo for checking out our data Eric, would be great to try out Migrate-n and see what results come of it. A couple other folks in the lab had similar interesting fst/structure results with their data (corals and fish) where O'ahu is more genetically similar to Kure/Midway atoll than to neighboring islands. Would be interesting to see if we get any cool results as Rob mentioned with military transport b/w O'ahu and Midway during WWII.
I can send along my TotalRawSNPs and Filtered vcf files through scp command if you have a destination path you can provide me. Please let me know what other info you need from me, happy to send along.

Eric:

>So it turns out that dDocent (or actually an associated perl script) can make haplotypes, but it will need the bam files for each individual. Supposedly we have unlimited storage on Google Drive at PSU, so I’m going to test that. I’ve shared a folder that you can use to upload the bam files.


# Transfer files

So, now I've received all .bam and .bai files for each individual, as well as locality metadata and total and filtered SNP datasets from 'Ale'a on Google Drive. I compressed these on my mac, and now need to transfer them to Argonaute. I will do this with the gdown python package (pip install gdown). This involves looking up the google id for each file in the URL of the share link and pasting that into gdown thusly

```{bash eval=F}
gdown https://drive.google.com/uc?id=19nagmzHPQgKE4PPlyHikcWc7lBZFMbXB --output ptuberculosa_metadata.csv
gdown https://drive.google.com/uc?id=19g9fMRdPH-_AG-l6kytu-lv6kd3qYpQD --output popmap
gdown https://drive.google.com/uc?id=1ASB4yOnwDzDhFPA7Pev4CyIl6M6SX8_P --output popmap.csv
gdown https://drive.google.com/uc?id=19ZsobdvBIs3bQukW0Q-ivpdlJEdGguns --output reference.fasta
gdown https://drive.google.com/uc?id=19XNrJvZKMQ3L7qTIuExajc5KY6-gj3P8 --output mm0.9minq30mac3thin500.recode.vcf
gdown https://drive.google.com/uc?id=19vth-3IMkfMfxq4cp0QLOUgC2yFqnZ3C --output TotalRawSNPs.vcf
gdown https://drive.google.com/uc?id=1I0ib-QX4eyedctD951os6gvAMUe761JU --output bam_files.zip


gdown https://drive.google.com/uc?id=1ES3yMzZS8JkWROiE1jdfTp6KBjxDdCXO --folder --remaining-ok --output firstbatch
```

# Haplotypes from dDocent

## Filter SNPs

'Ale' already filtered the SNPs quite a bit, but I need to start that process over because coalescent programs are sensitive to ascertainment bias. Also, need to knit them back into haplotypes. As Peter Beerli says on page 15 of the migrate-n manual:

>We use a rather restrictive ascertainment models for SNPs ?. A better approach than using SNPs is the use of short reads which may or many not contain SNPs. I find that SNPs are an inferior datatype because commonly researchers are adding criteria such as a minor SNP allele must occur at a frequency higher than x, and singletons are excluded etc.

> We have found ALL variable sites and use them even if there are only a few members of another alleles present. In principal it is as you would sequence a stretch of DNA and then remove the invariant sites. Each stretch is treated as completely linked. You can combine many of such “loci” to improve your estimates.


I've installed dDocent into a dedicated environment following instructions [here](https://www.ddocent.com/bioconda/). 

```{bash, eval = F}
conda activate ddocent_env
```

Following the tutorial, but altering filtering parameters. Keeping all SNPs with even a single occurence (Minor Allele Count = 1), but they must occur in 90% of individuals (mac 1) and be of high quality (minQ 30). We are keeping genotypes with as few as 3 reads. This is because evidence to keep these SNPs is based on their existence in multiple individuals.

```{bash, eval = F}
vcftools --vcf ../TotalRawSNPs.vcf --max-missing 0.1 --mac 1 --minQ 30 --minDP 3 --recode --recode-INFO-all --out rawg.1mac1dp3

```

>After filtering, kept 93 out of 93 Individuals
Outputting VCF file...
After filtering, kept 28128 out of a possible 52804 Sites
Run Time = 9.00 seconds

Run this error checking script to see the potential impact of shallow-sequenced genotypes

```{bash, eval = F}
ErrorCount.sh rawg.1mac1dp3.recode.vcf

<!-- This script counts the number of potential genotyping errors due to low read depth -->
<!-- It report a low range, based on a 50% binomial probability of observing the second allele in a heterozygote and a high range based on a 25% probability. -->
<!-- Potential genotyping errors from genotypes from only 1 read range from 0.0 to 0.0 -->
<!-- Potential genotyping errors from genotypes from only 2 reads range from 0.0 to 0.0 -->
<!-- Potential genotyping errors from genotypes from only 3 reads range from 3808.875 to 12797.82 -->
<!-- Potential genotyping errors from genotypes from only 4 reads range from 3860.25 to 19517.424 -->
<!-- Potential genotyping errors from genotypes from only 5 reads range from 913.78125 to 6930 -->
<!-- 93 number of individuals and 28128 equals 2615904 total genotypes -->
<!-- Total genotypes not counting missing data 2508832 -->
<!-- Total potential error rate is between 0.0034210765208670807 and 0.015642834593946504 -->
<!-- SCORCHED EARTH SCENARIO -->
<!-- WHAT IF ALL LOW DEPTH HOMOZYGOTE GENOTYPES ARE ERRORS????? -->
<!-- The total SCORCHED EARTH error rate is 0.04841934414101861. -->
```



Look at missing data per individual

```{bash, eval = F}
vcftools --vcf rawg.1mac1dp3.recode.vcf --missing-indv

mawk '!/IN/' out.imiss | cut -f5 > totalmissing
gnuplot << \EOF 
set terminal dumb size 120, 30
set autoscale 
unset label
set title "Histogram of % missing data per individual"
set ylabel "Number of Occurrences"
set xlabel "% of missing data"
#set yr [0:100000]
binwidth=0.01
bin(x,width)=width*floor(x/width) + binwidth/2.0
plot 'totalmissing' using (bin($1,binwidth)):(1.0) smooth freq with boxes
pause -1

```

```{bash, eval = F}
                                                                                                                        
                                         Histogram of % missing data per individual                                     
     20 +-----------------------------------------------------------------------------------------------------------+   
        |    * *      +            +             +            +             +            +             +            |   
        |    * **                                             'totalmissing' using (bin($1,binwidth)):(1.0) ******* |   
        |    * **                                                                                                   |   
        |    * **                                                                                                   |   
        |    * **                                                                                                   |   
     15 |-+  * **                                                                                                 +-|   
        |    * **                                                                                                   |   
        |   ** **                                                                                                   |   
        |   ** **                                                                                                   |   
        |   ** **                                                                                                   |   
     10 |-+ ** **                                                                                                 +-|   
        |   ** **                                                                                                   |   
        |   ** ***                                                                                                  |   
        |   ** ***                                                                                                  |   
        |   ** ***                                                                                                  |   
        |   ** *****                                                                                                |   
      5 |-+ ** *** *                                                                                              +-|   
        |  *** *** ****                                                                                             |   
        |  *** *** ** *                                                                                             |   
        |  *** *** ** *                                                                                             |   
        |  *** *** ** *******                                                                                       |   
        |***** *** ** *** * *******************************************************************************         |   
      0 +-----------------------------------------------------------------------------------------------------------+   
        0            0.1          0.2           0.3          0.4           0.5          0.6           0.7          0.8  
                                                      % of missing data                                                 
```

'Ale' has probably removed any really bad individuals already. I'll keep all of these for now.

Now filter based on missingness within populations. Puritz wrote a script for this that creates lots of output, so I created ./filterpop directory to run it in. I am removing loci that are missing 10% of data in any of 10 populations. I altered popmap to remove spaces from pop names too.

```{bash, eval=F}
pop_missing_filter.sh ../rawg.1mac1dp3.recode.vcf ../../popmap 0.1 10 rawg1mac1dp3allpops.1.vcf
```

This filter keeps loci with Allele Balance  between 0.25 and 0.75. From Jon:

>Allele balance is: a number between 0 and 1 representing the ratio of reads showing the reference allele to all reads, considering only reads from individuals called as heterozygous Because RADseq targets specific locations of the genome, we expect that the allele balance in our data (for real loci) should be close to 0.5

So this will keep only really high quality SNPs

```{bash, eval=F}
 vcffilter -s -f "AB > 0.25 & AB < 0.75 | AB < 0.01" ./filterpops/rawg1mac1dp3allpops.1.vcf.recode.vcf > rawg.1mac1dp3allpop.1.ABfil.vcf
 
 mawk '!/#/' *ABfil.vcf | wc -l
2756
```

Now remove SNPs that occur on both strands of a read. Jon: 

> Unless you are using super small genomic fragment or really long reads (MiSeq). A SNP should be covered only by forward or only reverse reads. The filter is based on proportions, so that a few extraneous reads won’t remove an entire locus.... In plain english, it’s keeping loci that have over 100 times more forward alternate reads than reverse alternate reads and 100 times more forward reference reads than reverse reference reads along with the reciprocal....That only removes a small proportion of loci, but these loci are likely to be paralogs, microbe contamination, or weird PCR chimeras.

```{basjh. eval = F}
vcffilter -f "SAF / SAR > 100 & SRF / SRR > 100 | SAR / SAF > 100 & SRR / SRF > 100" -s rawg.1mac1dp3allpop.1.ABfil.vcf > rawg.1mac1dp3allpop.1.ABfil.FRfil.vcf

mawk '!/#/' rawg.1mac1dp3allpop.1.ABfil.FRfil.vcf | wc -l
7
```

Hmmm... that's a bit too stringent... maybe these were done on a MiSeq?


> The next filter looks at the ratio of mapping qualities between reference and alternate alleles...The rationale here is that, again, because RADseq loci and alleles all should start from the same genomic location there should not be large discrepancy between the mapping qualities of two alleles.

```{bash eval =F}
vcffilter -f "MQM / MQMR > 0.9 & MQM / MQMR < 1.05" rawg.1mac1dp3allpop.1.ABfil.vcf > rawg.1mac1dp3allpop.1.ABfil.MQfil.vcf
mawk '!/#/' rawg.1mac1dp3allpop.1.ABfil.MQfil.vcf | wc -l
2512
```

>Yet another filter that can be applied is whether or not their is a discrepancy in the properly paired status of for reads supporting reference or alternate alleles....Since de novo assembly is not perfect, some loci will only have unpaired reads mapping to them. This is not a problem. The problem occurs when all the reads supporting the reference allele are paired but not supporting the alternate allele. That is indicative of a problem.

```{bash eval = F}
vcffilter -f "PAIRED > 0.05 & PAIREDR > 0.05 & PAIREDR / PAIRED < 1.75 & PAIREDR / PAIRED > 0.25 | PAIRED < 0.05 & PAIREDR < 0.05" -s rawg.1mac1dp3allpop.1.ABfil.MQfil.vcf > rawg.1mac1dp3allpop.1.ABfil.MQfil.MapFil.vcf

mawk '!/#/' rawg.1mac1dp3allpop.1.ABfil.MQfil.MapFil.vcf| wc -l
2141
```


```{bash eval = F}
dDocent_filters rawg.1mac1dp3allpop.1.ABfil.MQfil.MapFil.vcf rawg.1mac1dp3allpop.1.ABfil.MQfil.MapFil.dDFil.vcf 
This script will automatically filter a FreeBayes generated VCF file using criteria related to site depth,
quality versus depth, strand representation, allelic balance at heterzygous individuals, and paired read representation.
The script assumes that loci and individuals with low call rates (or depth) have already been removed. 

Contact Jon Puritz (jpuritz@gmail.com) for questions and see script comments for more details on particular filters 

Number of sites filtered based on allele balance at heterozygous loci, locus quality, and mapping quality / Depth
 1192 of 2141 

Are reads expected to overlap?  In other words, is fragment size less than 2X the read length?  Enter yes or no.
yes
Is this from a mixture of SE and PE libraries? Enter yes or no.
no
Number of additional sites filtered based on properly paired status
 0 of 949 

Number of sites filtered based on high depth and lower than 2*DEPTH quality score
 78 of 949 


                                                                                                                        
                                                                                                                        
                                               Histogram of mean depth per site                                         
       30 +---------------------------------------------------------------------------------------------------------+   
          |    +    +   **+    +    +    +     +    +    +    +     +    +    +    +     +    +    +    +     +    +|   
          |             **                                'meandepthpersite' using (bin($1,binwidth)):(1.0) ******* |   
          |         *** **                                                                                          |   
       25 |-+       * * **                                                                                        +-|   
          |        ** ****    **                                                                                    |   
          |        ** ****    **                                                                                    |   
          |        ** ****    **                                                                                    |   
       20 |-+   ** ** ****    **                                                                                  +-|   
          | **  ** ** ****    **                                                                                    |   
          | **  ** ** *****   ** **                                                                                 |   
       15 |***  ** ** *****   ** **                                                                               +-|   
          |**** ** ** *****   ** **                                                                                 |   
          |**** ** ** ******* ** **                                                                                 |   
          |**** ***** ******* ** **              **                                                                 |   
       10 |********** *************   ***        **                                                               +-|   
          |********** ************** ****     ** **                                                                 |   
          |********** ************** ****     ** **                                                                 |   
          |********** ********************** *** ***          *****                                                 |   
        5 |********** ******************** * ************     * ***                                               +-|   
          |********** ******************** **************  ** * *******  **                                         |   
          |********** ******************** ******************** ****************** ************ *** ***  ***       *|   
          |********** ******************** ******************** ************ *** *** ****** * *** *** **** *********|   
        0 +---------------------------------------------------------------------------------------------------------+   
          10   15   20    25   30   35   40    45   50   55   60    65   70   75   80    85   90   95  100   105  110   
                                                          Mean Depth                                                    
                                                                                                                        
If distrubtion looks normal, a 1.645 sigma cutoff (~90% of the data) would be 10141.7555
The 95% cutoff would be 101
Would you like to use a different maximum mean depth cutoff than 101, yes or no
no
Number of sites filtered based on maximum mean depth
 81 of 949 

Number of sites filtered based on within locus depth mismatch
 36 of 868 

Total number of sites filtered
 1309 of 2141 

Remaining sites
 832 

Filtered VCF file is called rawg.1mac1dp3allpop.1.ABfil.MQfil.MapFil.dDFil.vcf.FIL.recode.vcf

Filter stats stored in rawg.1mac1dp3allpop.1.ABfil.MQfil.MapFil.dDFil.vcf.filterstats
(ddocent_env) ecrandall@Argonaute:~/eric_data/ptuberculosa/filtering$ 
```

This script apparently does many of the other steps that I did above manually, so I started from an earlier checkpoint

```{bash, eval = F}
dDocent_filters  rawg1mac1dp3allpops.1.vcf.recode.vcf  rawg.1mac1dp3allpop.1.dDfil.vcf 
This script will automatically filter a FreeBayes generated VCF file using criteria related to site depth,
quality versus depth, strand representation, allelic balance at heterzygous individuals, and paired read representation.
The script assumes that loci and individuals with low call rates (or depth) have already been removed. 

Contact Jon Puritz (jpuritz@gmail.com) for questions and see script comments for more details on particular filters 

Number of sites filtered based on allele balance at heterozygous loci, locus quality, and mapping quality / Depth
 23877 of 25261 

Are reads expected to overlap?  In other words, is fragment size less than 2X the read length?  Enter yes or no.
yes
Is this from a mixture of SE and PE libraries? Enter yes or no.
no
Number of additional sites filtered based on properly paired status
 197 of 1384 

Number of sites filtered based on high depth and lower than 2*DEPTH quality score
 1673 of 1187 


                                                                                                                        
                                                                                                                        
                                               Histogram of mean depth per site                                         
       60 +---------------------------------------------------------------------------------------------------------+   
          | +    +     +    +     +    +    +     +    +     +    +     +    +    +     +    +     +    +     +    +|   
          |                                               'meandepthpersite' using (bin($1,binwidth)):(1.0) ******* |   
          |           **                                                                                            |   
       50 |-+         **                                                                                          +-|   
          |           **                                                                                            |   
          |        ** **                                                                                            |   
          |        ** **   **                                                                                       |   
       40 |-+      ** **   **                                                                                     +-|   
          |    **  ** **   **                                                                                       |   
          |    ** *** ***  ** **                                                                                    |   
       30 |-+  *********** *****                                                                                  +-|   
          | ** *********** *****                                                                                    |   
          | ** *****************                                                                                    |   
          | ********************                                                                                    |   
       20 |-********************  **                                                                              +-|   
          |********************** ***                                                                               |   
          |********************** ***                                                                               |   
          |****************************     *                                                                       |   
       10 |**************************** *** *                                                                     +-|   
          |******************************** **   **    ***** **                                                     |   
          |**************************************************** **       **         ***                             |   
          |************************************************************************** ****  **  ** +  *** **  +*****|   
        0 +---------------------------------------------------------------------------------------------------------+   
            12   18    24   30    36   42   48    54   60    66   72    78   84   90    96  102   108  114   120  126   
                                                          Mean Depth                                                    
                                                                                                                        
If distrubtion looks normal, a 1.645 sigma cutoff (~90% of the data) would be 11589.30825
The 95% cutoff would be 116
Would you like to use a different maximum mean depth cutoff than 116, yes or no
no
Number of sites filtered based on maximum mean depth
 109 of 1187 

Number of sites filtered based on within locus depth mismatch
 21 of 1072 

Total number of sites filtered
 24210 of 25261 

Remaining sites
 1051 

Filtered VCF file is called rawg.1mac1dp3allpop.1.dDfil.vcf.FIL.recode.vcf

Filter stats stored in rawg.1mac1dp3allpop.1.dDfil.vcf.filterstats

mv rawg.1mac1dp3allpop.1.dDfil.vcf.FIL.recode.vcf rawg.1mac1dp3allpop.1.dDfil.vcf
```

Now to filter by HWE

>The next filter to apply is HWE. Heng Li also found that HWE is another excellent filter to remove erroneous variant calls. We don’t want to apply it across the board, since population structure will create departures from HWE as well. We need to apply this by population. I’ve included a perl script written by Chris Hollenbeck, one of the PhD student’s in my current lab that will do this for us.

>Let’s filter our SNPs by population specific HWE First, we need to convert our variant calls to SNPs To do this we will use another command from vcflib called vcfallelicprimatives

```{bash, eval =F}
vcfallelicprimitives rawg.1mac1dp3allpop.1.dDfil.vcf --keep-info --keep-geno > rawg.1mac1dp3allpop.1.dDfil.prim.vcf
```

> This will decompose complex variant calls into phased SNP and INDEL genotypes and keep the INFO flags for loci and genotypes. Next, we can feed this VCF file into VCFtools to remove indels.

```{bash, eval =F}
vcftools --vcf rawg.1mac1dp3allpop.1.dDfil.prim.vcf --remove-indels --recode --recode-INFO-all --out SNP.rawg.1mac1dp3allpop.1.dDfil
```

>Outputting VCF file...
After filtering, kept 1096 out of a possible 1126 Sites
Run Time = 0.00 seconds

Now apply the HWE filter, in filterhwe folder

```{bash, eval = F}
filter_hwe_by_pop.pl -v ../SNP.rawg.1mac1dp3allpop.1.dDfil.recode.vcf -p ../../popmap -o SNP.rawg.1mac1dp3allpop.1.dDfil.HWE -h 0.01 -c 0.1

<!-- Processing population: BigIsland (17 inds) -->
<!-- Processing population: FrenchFrigateShoals (9 inds) -->
<!-- Processing population: Kauai (15 inds) -->
<!-- Processing population: Kure (10 inds) -->
<!-- Processing population: MaroReef (5 inds) -->
<!-- Processing population: Maui (10 inds) -->
<!-- Processing population: Molokai (10 inds) -->
<!-- Processing population: Oahu (10 inds) -->
<!-- Processing population: Pearl_Hermes (4 inds) -->
<!-- Processing population: PioneerBanks (3 inds) -->
<!-- Outputting results of HWE test for filtered loci to 'filtered.hwe' -->
<!-- Kept 997 of a possible 1096 loci (filtered 99 loci) -->
```


Hmm... the HWE filter is not working great, likely because of low sample sizes. It's leaving me with mostly loci without individuals with alternate homozygotes 

```{bash, eval = F}
conda deactivate
```

## Haplotyping

### rad_haplotyper.pl

Chris Hollenbeck has created a script that can get haplotypes from dDocent output. First, gotta remove the -RG from the .bam and .bai files.

```{bash, eval = F}
for f in *.bam; do mv "$f" "${f%-RG.bam}.bam" ; done
for f in *.bam.bai; do  mv "$f" "${f%-RG.bam.bai}.bam.bai" ; done
```

Now, to install the rad_haplotyper environment following [these](https://github.com/chollenbeck/rad_haplotyper) instructions

hmmm ... rad_haplotyper isn't working because the Vcf Perl module changed its name to VCF... breaking the script. I tried a few minor modifications but couldn't fix it. Created an issue in the github site.

Remember to remove the environment: $ conda remove rad_haplotyper_env



### Microhaplot

I am now going to try [microhaplot](https://ngthomas.github.io/microhaplot/index.html) by Thomas Ng and Eric Anderson!!

Now, have to convert the bam files to sam files.

```{bash eval = F}
for bam in *.bam                                                                                                      
do
echo $bam
samtools view -h -o ../sam_files/${bam%.bam}.sam $bam
done

for bam in *.bam                                                                                                      
do
echo $bam
mv $bam ${bam%-RG.bam}.bam
done

for bam in *.bai                                                                                                      
do
echo $bam
mv $bam ${bam%-RG.bam.bai}.bam.bai
done

for bam in *.bam                                                                                                      
do
echo $bam
samtools idxstats $bam > ./idxstats/${bam%.bam}_idxstats.txt
done


for bam in *.bam                                                                                                      
do
echo $bam
samtools view -h -o /Users/edc5240/Datasets/Ptuberculosa/sam_files/${bam%.bam}.sam $bam
done
```

#### Tutorial

Do the [tutorial](https://ngthomas.github.io/microhaplot/articles/microhaplot-walkthrough.html)

```{r, eval = F}
library(microhaplot)
shiny_dir <- "/Volumes/GoogleDrive-103325533945714033178/My Drive/Ptuberculosa/Ptuberculosa_data/shiny_dir"
microhaplot::mvShinyHaplot(shiny_dir)

app.path <- file.path(shiny_dir, "microhaplot")
microhaplot::runShinyHaplot(app.path)
```


#### Load Data
Then follow along [here](https://ngthomas.github.io/microhaplot/articles/microhaplot-data-prep.html) to get data into microhaplot. 

```{r, eval = F}
library(microhaplot)

run.label <- "Ptuberculosa"

path <- "/Users/edc5240/Datasets/Ptuberculosa"
sam.path <- "/Users/edc5240/Datasets/Ptuberculosa/sam_files"
# untar(system.file("extdata",
#                   "sebastes_sam.tar.gz",
#                   package="microhaplot"),
#       exdir = sam.path)


label.path <- file.path(path, "labels.txt")
vcf.path <- file.path(path, "mm0.9minq30mac3thin500.recode.vcf")

mvShinyHaplot(file.path(path,"shiny_dir"))
app.path <- file.path(path,"shiny_dir", "microhaplot")

haplo.read.tbl <- prepHaplotFiles(run.label = run.label,
                            sam.path = sam.path,
                            out.path = path,
                            label.path = label.path,
                            vcf.path = vcf.path,
                            app.path = app.path)

runShinyHaplot(path = "/Users/edc5240/Datasets/Ptuberculosa/shiny_dir/microhaplot")
```

Working with the data in microhaplot was OK, but I kept getting unexplained N's in the output, and it was going to take some coding effort to translate the output into migrate format. Moreover, microhaplot only kept the variable sites, which isn't ideal for migrate's mutation model. So I am giving up on this front, and am going to try to generate haplotypes with Stacks.

# Haplotypes from Stacks

## Setup

Need first to copy all the fastq files over from Google Drive, where 'Ale'a put them. I split them into 4 tranches to keep the number of files under 50 as required by gdown. Note that the folder protocol won't take urls, but just the ID.

```{bash eval = F}
gdown --id 1ES3yMzZS8JkWROiE1jdfTp6KBjxDdCXO --folder --remaining-ok --output firstbatch
gdown --id 1Ofta4RLcf6vddIiAA6dLCkdrYngADW1d --folder --remaining-ok --output secondbatch
gdown --id 1Og6JQNlhgprYvLTxJtMBlCCj72qRxLOw --folder --remaining-ok --output thirdbatch
gdown --id 1Ojnlj3L_8UFoPUVxaqvLKTLs8qq_S9ml --folder --remaining-ok --output fourthbatch

```


Need to rename the files into Illumina format in order for Stacks to recognize the paired reads

```{bash eval = F}
cd raw

for f in *.F.fq.gz
do
mv $f ${f/_1.F.fq.gz/_R1_001.fastq.gz}
done

for f in *.R.fq.gz
do
mv $f ${f/_1.R.fq.gz/_R2_001.fastq.gz}
done

cd ..
```

## Process RadTags

'Ale'a said:

>It was an ezRAD protocol developed by the Tobo lab. The restriction enzymes used were both MboI and Sau3AI. The sequences have indexes removed but still need to be trimmed/ends cleaned.

So I will process the radtags to remove adapters and the sau3AI cut site (GATC, which is the same as MboI).
This command cleans data removing reads with uncalled bases, low phred scores, rescue barcodes and rad-tags.

```{bash eval = F}

process_radtags -p ./raw/ -o ./cleaned/ --rescue --clean --quality --paired --filter_illumina --renz-1 sau3AI \
--adapter-1 AGATCGGAAGAGCACACGTCTGAACTCCAGTCA \
--adapter-2 AGATCGGAAGAGCGTCGTGTAGGGAAAGAGTGT

```

> 724964572 total sequences
        0 failed Illumina filtered reads (0.0%)
227835784 reads contained adapter sequence (31.4%)
        0 barcode not found drops (0.0%)
 28941251 low quality read drops (4.0%)
 63374437 RAD cutsite not found drops (8.7%)
404813100 retained reads (55.8%)

## Pipeline


Hmm... gotta rename the cleaned files too

```{bash eval = F}
cd cleaned
for f in *001.1.fq.gz
do
mv $f ${f/_R[12]_001.1.fq.gz/.1.fq.gz}
done

for f in *001.2.fq.gz
do
mv $f ${f/_R[12]_001.2.fq.gz/.2.fq.gz}
done
```


I'm going to use the denovo_map.pl pipeline. This command will take data from `./cleaned` and the popmap I created for dDocent and -m 3 reads per stack, -n 4 distance between stacks, -M 4 distance between catalog loci. Running on 12 -T threads and only keeping loci that appear in 75% of individuals in all 10 populations

```{bash eval = F}
denovo_map.pl --samples ./cleaned/ --popmap ../popmap --out-path ./pipeline --paired \
-m 3 -n 4 -M 4 -T 12 -r 75 -p 10 -X "populations: --fasta-samples" -X "populations: --filter-haplotype-wise"
```

Copy it down

```{bash eval = F}
scp -r  ecrandall@128.118.123.64:eric_data/ptuberculosa/stacks/pipeline/output1 ./stacks_output              
```

## Populations

Now I need to re-run `populations` to get more statistics. Original populations output is in output1, this will be in output2.

```{bash eval = F}
populations -P ../ -O ./ -M ../../../popmap -t 12 -p 10 -r 75 -H --hwe --fstats --p-value-cutoff 0.99 --fasta-loci --fasta-samples --vcf --genepop --structure --treemix --fasta-samples-raw 
```

## Fasta2Genotype

Paul Maier created [this script](http://www.mountainmanmaier.com/software/) to convert Stacks haplotypes to migrate format. I had to remove all the [individualName] tokens from the populations.samples.fa to get it to work.

```{bash eval = F}
python2 /Applications/migrate/migrate-4.4.4/fasta2genotype/fasta2genotype.py populations.samples2.fa NA ../popmap.tsv  populations.snps.vcf ptuberculosa.mig

```

```{bash, eval = F}
###################################################################
###                                                             ###
###       Fasta2Genotype | Data Conversion | Version 1.10       ###
###                                                             ###
###                        Cite as follows:                     ###
###                                                             ###
###   Maier P.A., Vandergast A.G., Ostoja S.M., Aguilar A.,     ###
###   Bohonak A.J. (2019). Pleistocene glacial cycles drove     ###
###   lineage diversification and fusion in the Yosemite toad   ###
###   (Anaxyrus canorus). Evolution, in press.                  ###
###   https://www.doi.org/10.1111/evo.13868                     ###
###                                                             ###
###################################################################

Output type? [1] Migrate [2] Arlequin [3] DIYABC [4] LFMM [5] Phylip [6] G-Phocs [7] Treemix [8] Haplotype: 1
Remove restriction enzyme or adapter sequences? These may bias data. [1] Yes [2] No: 2
Coverage Cutoff (number reads for locus)? Use '0' to ignore coverage: 0
Remove monomorphic loci? [1] Yes [2] No: 2
Remove loci with excess heterozygosity? This can remove paralogs. [1] Yes [2] No: 1
Maximum heterozygosity cutoff for removing loci out of Hardy-Weinberg? 0.5
Filter for allele frequency? False alleles might bias data. [1] Yes [2] No: 2
Filter for missing genotypes? These might bias data. [1] Yes [2] No: 2
 
**************************************************************************************************************
***                                       ... BEGINNING CONVERSION ...                                     ***
**************************************************************************************************************
 
Cataloging loci...
Counting locus lengths...
Cataloging populations...
Counting gene copies...
Counting alleles for each locus...
Identifying loci with excess heterozygosity...
     Calculating observed heterozygosity and homozygosity...
     Calculating expected heterozygosity and homozygosity...
     Flagging loci with excess heterozygosity for removal...
     Removing loci...
     Removed 2 overly heterozygous loci.
Outputting migrate-n file...
*** DONE! ***
```

Then I manually re-ordered the populations to run from north to south in the output file, and moved it into the migrate folder.

# Migrate

## Install Migrate

Install the parallel version of Migrate on Nautilus server. I hade to remove `-fvectorize` from line 100 of the makefile to get this to work.

```{bash eval = F}
curl https://peterbeerli.com/migrate-html5/download_version4/migrate-newest-src.tar.gz > migrate.tar.gz
tar -fvxz migrate.tar.gz
cd migrate-4.4.4/
cd src
./configure
make mpis (for parallel running)
sudo cp migrate-n-mpi /usr/local/bin/migrate-n-mpi
```

##  Locus Statistics & Mutation Model

These RAD loci are much less variable than COI that I'm used to using. I need to figure out an overall mutation model to use with RAD loci. I can't find any [discussion of this issue on the Migrate Google Group](https://groups.google.com/g/migrate-support/search?q=%22mutation%20model%22). I guess I'll use `modelTest` in the phangorn package to see where that gets me.

I renamed the FASTA headers in `populations.samples.fa` with BBEdit:

```{bash eval = F}
Find: >CLocus_\d+_Sample_\d+_Locus_(\d+)_Allele_([01]) \[(.+)\]
Replace: >\3_L\1_A\2

```

Lets load in the data and calculate some statistics for each locus. Previously Migrate-n only implemented the F84 (=HKY) model, with two rates (Transistions and Transversions) and gamma distributed rate variability. The new v4 has a `datamodel` parameter that suggests it might take other models of molecular evolution, but the possible models are not listed in the documentation! So I will just fit an HKY model. 

(note the interface lists these possible models: 
 1  Jukes-Cantor model
 2  Kimura 2-parameter model
 3  Felsenstein 1981 (F81) model
 4  Felsenstein 1984 (F84) model
 5  Hasegawa-Kishino-Yano model
 6  Tamura-Nei model
) - I may try to code up tests for these down the line.

```{r stats, warning = F, eval=F}

fastadata <- read.FASTA("/Volumes/GoogleDrive/My Drive/Ptuberculosa/stacks_output/allpops75_hwe/populations.samples_rename.fasta")

# Get locus names
locus_names <- str_extract(names(fastadata),"L\\d+") %>% unique()

stats <- tibble(locus = character(), 
                length = numeric(),
                segSites = numeric(),
                nHaps = numeric(),
                nucDiv = numeric(),
                ttRatio = numeric(),
                gammaShape = numeric(), 
                rate1 = numeric(), 
                rate2 = numeric(), 
                rate3 = numeric(),
                rate4 = numeric())
                
for(l in locus_names){
  print(l)
  locus_dnabin <- fastadata[str_which(names(fastadata),pattern = l)]
  # convert to package formats
  locus_dnabin <- as.matrix(locus_dnabin)
  locus_gtypes <- sequence2gtypes(locus_dnabin)
  locus_phy <- phyDat(locus_dnabin)
  #create a haplotype network .. to be continued
  haps <- haplotype(locus_dnabin)
  nhaps <- length(dimnames(haps)[[1]])
  #tcs <- haploNet(haps)
  #find parameters of HKY (F84) model
 # modeltest <- modelTest(locus_phy, model = c("JC","K80", "F81", "HKY"), 
  #                       G = T, I = F, k = 4)
  HKY <- modelTest(locus_phy, model = c("HKY"), 
                         G = T, I = F, k = 4)
  env <- attr(HKY, "env")
  HKYG <- get("HKY+G", env)
  model <- eval(HKYG, env=env)
  # calculate TiTv Ratio
  ttratio <- TiTvRatio(locus_gtypes)
  
  stats <- bind_rows(stats, tibble(locus=l, 
                          length = length(locus_dnabin[1,]),
                          segSites = length(seg.sites(locus_dnabin)),
                          nHaps = length(dimnames(haps)[[1]]),
                          nucDiv = nuc.div(locus_dnabin),
                          ttRatio =  ttratio[3],
                          gammaShape = model$shape,
                          rate1 = model$g[1],
                          rate2 = model$g[2],
                          rate3 = model$g[3],
                          rate4 = model$g[4] ))
                        
}
# write_csv(stats, "./migrate/run2_locus_statistics.csv")
```
Read them back in so we don't have to recalculate them every time I knit.

```{r}
stats <- read_csv("./migrate/run2_locus_statistics.csv")
kable(stats)
```

So we have a `r length(which(stats$segSites == 0))` invariant loci, and the mean overall transition:transversion ratio is `r mean(!is.infinite(stats$ttRatio),na.rm = T)`. Mean gamma shape parameter is `r mean(stats$gammaShape)`, which argues for only one rate.

## Parmfile

I went through and compared my 3.x parmfile line by line to the default one generated by Migrate 4.4.4, and came up with this

```{bash parmfile, eval = F}
################################################################################
# Parmfile for Migrate 4.4.4(git:v4-series-26-ge85c6ff)-June-1-2019 [do not remove these first TWO lines]
# generated automatically on
# Sat Jan 15 14:10:36 2022
#
# please report problems to Peter Beerli
#  email: beerli@fsu.edu
#  http://popgen.sc.fsu.edu/migrate.html
menu=YES
nmlength=10
datatype=SequenceData
datamodel=HKY
ttratio=1.000000

freqs-from-data=YES 

# data were filtered to Q30, so this seems like a good seqerror
seqerror-rate={0.0001,0.0001,0.0001,0.0001}
categories=1 #no categories file specified
rates=1: 1.000000 
prob-rates=1: 1.000000 
autocorrelation=NO
weights=NO
recover=NO
fast-likelihood=NO
inheritance-scalars={1.00000000000000000000}
haplotyping=YES:no-report
population-relabel={1 2 3 4 5 6 7 8 9 10}
infile=../../ptuberculosa.mig
random-seed=AUTO #OWN:410568459
title= Palythoa tuberculosa - Hawaii

progress=YES

logfile=NO

print-data=NO

outfile=outfile.txt

pdf-outfile=outfile.pdf

pdf-terse=NO

use-M=YES

print-tree=NONE

mig-histogram=NO
skyline=NO #needs mig-histogram=ALL:...

theta=PRIOR:50
migration=PRIOR:10
rate=PRIOR:50
split=PRIOR:10
splitstd=PRIOR:10

mutation=CONSTANT

analyze-loci=A

divergence-distrib=S

#standard stepping-stone model for starters
custom-migration={
**00000000
***0000000
0***000000
00***00000
000***0000
0000***000
00000***00
000000***0
0000000***
00000000**
}

geo=NO

updatefreq=0.200000 0.200000 0.200000 0.200000 #tree, parameter haplotype, timeparam updates
bayes-posteriorbins= 500 500
bayes-posteriormaxtype=TOTAL
bayes-file=YES:bayesfile
bayes-allfile=YES:bayesallfile
bayes-all-posteriors=YES:bayesallposterior
bayes-proposals= THETA METROPOLIS-HASTINGS Sampler
bayes-proposals= MIG SLICE Sampler
bayes-proposals= DIVERGENCE METROPOLIS-HASTINGS Sampler
bayes-proposals= DIVERGENCESTD METROPOLIS-HASTINGS Sampler
bayes-priors= THETA WEXPPRIOR: 0.0 0.01 0.1000000 0.01000 
bayes-priors= MIG WEXPPRIOR: 0.000100 100000.000000 1000000
bayes-priors= RATE * * UNIFORMPRIOR: 0.000000 10000000000.000000 1000000000.000000 
bayes-hyperpriors=NO
#
long-chains=1
long-inc=100
long-sample=10000
burn-in=2000  
auto-tune=YES:0.440000
assign=NO

heating=YES:1:{1.000000,1.500000,3.000000,1000000.000000}
heated-swap=YES

moving-steps=NO

gelman-convergence=No

replicate=NO

end


```

## Test Run

Let's see what happens! Started this run on Jan 22, at 10pm.

```{bash testrun, eval = F}
screen -S migrate_testrun

mpirun -np 32 ~/migrate-4.4.4/src/migrate-n-mpi parmfile

```

And it was finished the next morning!
Prior on m was too wide, giving results like this:

![Wide Priors on m](./figures/wide_priors.jpg)
No bueno.

### Revise m prior

Revised the m prior like so (I had no window value in the original!) Order of values is min, mean, maximum, proposal window.

```{bash eval = F}
bayes-priors= MIG WEXPPRIOR: 0.000100 1000.000000 10000 100
```



### Panmixia

A few modifications to make a panmictic model

```{bash eval = F}
population-relabel={1 1 1 1 1 1 1 1 1 1}
#and
custom-migration={
*
}
```

### Island Model

Change all parameter values to m for island model.

```{bash eval = F}
custom-migration={
mmmmmmmmmm
mmmmmmmmmm
mmmmmmmmmm
mmmmmmmmmm
mmmmmmmmmm
mmmmmmmmmm
mmmmmmmmmm
mmmmmmmmmm
mmmmmmmmmm
mmmmmmmmmm
}
```

Minor github issue. Got a little hung up at this point because I accidentally committed some bayesallfiles that were hundreds of Mb, and of course github wouldn't let me upload those. Thought I fixed it with [these instructions](https://stackoverflow.com/questions/12481639/remove-files-from-git-commit). Then ended up trying [git-filter-repo](https://htmlpreview.github.io/?https://github.com/newren/git-filter-repo/blob/docs/html/git-filter-repo.html) (installed via homebrew), then ended up restoring the whole thing from the github repository. Ouch. Then it wouldn't push until I ran `git prune`

### Results

Very nice! `r `ji("smiley")` Going to need to modify my code to read multilocus output from migrate-n.

```{r run1_results}
workingDir <- "./migrate/run2"

modelMarglikes <- harvest.model.likelihoods(workingDir = workingDir)


kable(modelMarglikes, format.args = list(digits = 5))

results <- bfcalcs(modelMarglikes)

kable(results)
```
